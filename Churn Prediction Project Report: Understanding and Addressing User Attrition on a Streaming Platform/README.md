Churn Prediction Project Report: Understanding and Addressing User Attrition on a Streaming Platform
1. Introduction: Navigating the Tides of User Retention
In the dynamic and highly competitive landscape of streaming services, user churn ,the rate at which customers discontinue their subscriptions ,represents a significant challenge to sustained growth and profitability. This report details a comprehensive churn prediction project undertaken to equip our retention team with the insights and tools necessary to proactively identify at-risk customers and implement targeted strategies to mitigate subscriber loss. Our primary business goal is not merely to understand churn but to predict it with sufficient accuracy to allow for timely intervention, thereby enhancing user lifetime value and fortifying our platform's subscriber base.
The foundation of this analysis is a rich dataset encompassing a wide array of user information. This includes essential demographics, crucial engagement metrics such as watch time and mobile application usage patterns, and vital account metadata, including subscription types and promotional histories. A significant advantage of our dataset is that categorical variables, like the user's country of origin and their chosen subscription tier, have already been meticulously one-hot encoded. This pre-processing step transformed these qualitative attributes into a numerical format, making them directly amenable to machine learning algorithms.
The purpose of this analysis is multi-faceted. Firstly, we aimed to identify clear behavioral indicators of churn, seeking to understand why users decide to leave the platform. Secondly, armed with these insights, we endeavored to build robust predictive models capable of forecasting which users are most likely to churn in the near future. Thirdly, a critical objective was to segment users by their churn likelihood, allowing the retention team to allocate resources effectively and tailor their outreach. Finally, and perhaps most importantly, the project sought to recommend actionable retention strategies directly derived from our findings, ensuring that the insights translate into tangible business improvements.

2. Data Cleaning Summary: Refining the Raw Material
The journey from raw data to actionable intelligence always begins with meticulous data cleaning. This crucial phase ensures the integrity and usability of the dataset for subsequent analysis and model building. Our initial step involved identifying and dropping columns deemed irrelevant to the prediction of churn. Specifically, user_id, signup_date, and last_active_date were removed. While user_id serves as a unique identifier, it offers no predictive power regarding churn. Similarly, signup_date and last_active_date were considered for removal as their direct presence could introduce data leakage (a phenomenon where information from the future "leaks" into the training data, leading to overly optimistic model performance), especially when constructing time-dependent features like user tenure. However, it's important to note that elements derived from these dates, such as tenure, are highly valuable, and their calculation was carefully managed to prevent leakage.
Next, we focused on converting datatypes where necessary. For instance, although signup_date was initially dropped, its underlying date information was crucial for calculating tenure_days. Had it been kept as an object or string, it would have been converted to a datetime object before any time-based computations. This conversion ensures that temporal calculations are performed accurately and efficiently.
A significant aspect of data preparation involved handling missing values. Missing data can severely impair model performance and introduce biases. To address this, we employed SimpleImputer(strategy='mean'). This approach replaces any missing numerical values within a column with the mean of that column. While simple, this strategy is effective for many datasets, particularly when the proportion of missing values is not excessively high. It ensures that no valuable rows are discarded merely due to isolated missing entries, preserving as much information as possible for the models.
Finally, for the specific requirements of our machine learning models, we filtered the dataset to include only numeric columns for model input. This step is standard practice as most traditional machine learning algorithms operate on numerical data. As previously mentioned, categorical features like country and subscription_type were already one-hot encoded within the dataset. This meant they were already represented as numerical binary columns (e.g., country_USA, country_Canada, where a 1 indicates presence and a 0 indicates absence), alleviating the need for further encoding at this stage of the pipeline. This meticulous cleaning process ensured our data was robust, consistent, and ready for feature engineering and model training.

3. Feature Engineering Summary: Sculpting Predictive Power
Beyond merely cleaning the data, feature engineering is a creative and critical process of transforming raw data into features that better represent the underlying problem to the predictive models, thereby improving their accuracy. Our feature engineering efforts yielded several powerful new variables that significantly enhanced our understanding of churn.
One of the most intuitive and impactful features derived was tenure_days. This metric quantifies the duration a user has been active on the platform, calculated as the number of days between their original signup date and their last recorded active date. A longer tenure often suggests greater user satisfaction and stickiness, making it a strong negative predictor of churn. The calculation was performed carefully to ensure that last_active_date was treated as an observation point, not a future event, thereby avoiding data leakage.
Building upon tenure_days, we engineered a boolean feature called is_loyal. This binary variable flags users with a tenure exceeding 180 days (approximately six months). The hypothesis here was that users who have committed to the platform for a substantial period are inherently more loyal and, consequently, less prone to churn. This feature helps the models distinguish between newer users, who might be more susceptible to early churn, and established users with deeper engagement.
While the original dataset had country and subscription_type already one-hot encoded into numerous binary columns (e.g., country_USA, country_UK, subscription_basic, subscription_premium), for interpretive purposes and sometimes for certain modeling techniques, we reconstructed single country and subscription_type categorical columns from these one-hot encoded representations. This process involved identifying which of the one-hot columns had a value of 1 for each row, thereby inferring the original categorical label. While the models primarily used the one-hot encoded versions, having the original categorical forms facilitated clearer human understanding of the data and some exploratory analyses.
Finally, to prepare the features for our machine learning models, we derived model-ready inputs by scaling and imputing any remaining missing values. Scaling, typically using methods like StandardScaler or MinMaxScaler, transforms numerical features to a common range. This is crucial for algorithms sensitive to feature magnitudes, such as Logistic Regression. Even after initial missing value handling, a final imputation step ensures that no NaNs remain before feeding data to the algorithms, which generally cannot handle them. This comprehensive feature engineering process ensured that our models were trained on the most informative and appropriately formatted data possible.

4. Key Findings: Unveiling the Drivers of Churn
Our in-depth analysis revealed several compelling insights into the factors influencing user churn on the streaming platform. These key findings provide a data-driven foundation for developing effective retention strategies.
One of the most statistically significant findings pertained to the impact of promotions. Our analysis revealed that promotions significantly reduce churn, with a p-value less than 0.05 (p<0.05). This statistical significance indicates that the observed reduction in churn among users receiving promotions is highly unlikely to be due to random chance. This strongly suggests that promotional offers act as a powerful incentive for users to remain subscribed to the service.
Another crucial insight was the relationship between watch hours and retention. We observed a clear and strong correlation between higher watch hours and increased retention. Users who spend more time consuming content on the platform are demonstrably less likely to churn. This reinforces the intuitive idea that content engagement is a primary driver of customer loyalty.
Conversely, our analysis indicated that mobile-dominant users churn slightly more. While the correlation was not as strong as watch hours or promotions, it suggests that users who primarily access the platform via mobile devices exhibit a marginally higher propensity to cancel their subscriptions. This finding prompts further investigation into the mobile user experience.
Beyond these quantitative measures, we also identified important behavioral trends associated with churn. It was evident that users with high complaint counts and consistently low engagement metrics are more likely to churn. This highlights the importance of customer satisfaction and active platform usage as indicators of retention risk. Furthermore, an interesting social dynamic emerged: referral by a friend appears to help retain users. This suggests that word-of-mouth acquisition not only brings in new users but also potentially fosters a stronger initial connection to the platform, leading to greater loyalty. These findings collectively paint a detailed picture of the user behaviors and platform interactions that underpin churn.

5. Model Results: Quantifying Predictive Power
The core of our churn prediction project involved building and evaluating machine learning models. We focused on two primary algorithms: Logistic Regression and Random Forest, assessing their performance using key metrics such as F1 Score and Area Under the Receiver Operating Characteristic (ROC) Curve (AUC).
Our initial model was a Logistic Regression classifier. To optimize its performance, we employed GridSearchCV, a powerful technique for hyperparameter tuning. GridSearchCV systematically searches through a predefined grid of hyperparameter values, using cross-validation to find the combination that yields the best model performance. After this tuning process, our Logistic Regression model achieved an F1 Score of 0.76. The F1 Score is the harmonic mean of precision and recall, providing a balanced measure of a model's accuracy, especially valuable in situations with class imbalance. It reflects the model's ability to correctly identify churners while minimizing false positives and false negatives. Furthermore, the Logistic Regression model yielded an AUC of 0.81.
As a robust baseline and often a more powerful non-linear model, we also implemented a Random Forest classifier. The Random Forest model demonstrated superior performance, achieving an F1 Score of 0.79. This improvement over Logistic Regression suggests that the Random Forest, with its ensemble nature and ability to capture complex non-linear relationships, was better suited to the nuances of our dataset. Its AUC was 0.85, further solidifying its strong predictive capabilities.
ROC Curve Explanation: A Visual Testament to Predictive Strength
The ROC Curve (Receiver Operating Characteristic Curve) is a fundamental visualization for evaluating the performance of classification models, particularly when dealing with imbalanced datasets. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. The AUC (Area Under the Curve) metric quantifies the overall performance, representing the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative instance.
In our case, an AUC of 0.85 for the Random Forest model is a highly encouraging result. It signifies a strong model separation ability, meaning the model is very good at distinguishing between churners and non-churners. The fact that the ROC curve lies significantly above the diagonal line (the line representing random guessing) emphatically confirms the predictive power of our model. An AUC of 1.0 represents a perfect classifier, while an AUC of 0.5 indicates a classifier no better than random chance. Our 0.85 AUC places the model firmly in the "good" to "excellent" predictive range, providing a high degree of confidence in its ability to identify at-risk users.
Top 3 Predictors of Churn: Unpacking Feature Importance
Understanding which features contribute most significantly to the model's predictions is crucial for developing targeted interventions. Our analysis identified the top 3 predictors of churn:
1.	average_watch_hours: This feature emerged as the most influential predictor. The relationship is inverse: lower average watch hours strongly correlate with a higher likelihood of churn. This is intuitively sound; users who are not actively engaging with the content are less likely to perceive value from their subscription.
2.	received_promotions: The receipt of promotions was the second most impactful feature, demonstrating that receiving promotions significantly reduces churn risk. This validates our earlier statistical finding and emphasizes the effectiveness of promotional campaigns in fostering retention.
3.	mobile_app_usage_pct: The third key predictor was the percentage of platform usage attributed to the mobile application. Our models indicated that higher mobile usage shows a correlation with higher churn. While this relationship is not as strong as the first two, it suggests potential friction points or less satisfying experiences for users who primarily engage with the platform via mobile devices. This finding warrants further qualitative investigation into the mobile user experience.
These insights into feature importance provide a clear roadmap for the retention team, highlighting the levers they can pull to impact churn.

6. Business Questions Answered: Translating Data into Strategy
The rigorous analysis and model building were directly geared towards answering critical business questions, providing actionable insights for the retention team.
1.	Do users who receive promotions churn less?
Yes, our analysis definitively confirms this. There is a statistically significant difference in churn rates between users who receive promotions and those who do not, with promotional recipients exhibiting lower churn. This suggests that discounts, special offers, or exclusive content access can be powerful retention tools.
2.	Does watch time impact churn likelihood?
Yes, absolutely. Our findings show a clear inverse relationship: more watch time correlates directly with a lower churn likelihood. This underscores the paramount importance of content engagement in retaining subscribers. Users who are immersed in the platform's offerings are less likely to discontinue their service.
3.	Are mobile-dominant users more likely to cancel?
Yes, our models indicate a slight positive correlation with churn for users who primarily access the platform through mobile applications. While not as strong a driver as watch time or promotions, this suggests a potential area for improvement within the mobile user experience, perhaps related to app performance, feature accessibility, or content discovery on smaller screens.
4.	What are the top 3 features influencing churn?
Based on our model's feature importance analysis, the three most influential features predicting churn are:
o	average_watch_hours (lower hours indicate higher churn risk)
o	received_promotions (receiving promotions reduces churn risk)
o	mobile_app_usage_pct (higher mobile usage shows a slight positive correlation with churn)
5.	Which customer segments should the retention team prioritize?
Our analysis points to three key customer segments that the retention team should prioritize for targeted interventions:
o	Users with consistently low watch time: These individuals are at high risk as they are not engaging sufficiently with the core product.
o	Users exhibiting high mobile usage: While the correlation is slight, this segment warrants attention, perhaps through enhanced mobile-specific engagement strategies or UX improvements.
o	Users who have not been receiving promotions: This segment represents an opportunity to leverage a proven churn reduction tactic.
These clear answers provide a strategic framework for the retention team, allowing them to focus their efforts on the most impactful areas.

7. Recommendations: Forging a Path to Enhanced Retention
Based on our comprehensive analysis and the insights gleaned from the predictive models, we propose three key recommendations to the retention team, designed to directly address the identified drivers of churn and improve overall subscriber retention.
1.	Boost promotions to users with short tenure or low engagement.
Given the strong statistical evidence that promotions significantly reduce churn, a proactive strategy would be to increase the frequency and targeting of promotional offers to users who exhibit early warning signs. This includes new users with short tenure who might be in their trial period or initial months and are still evaluating the platform's value. Similarly, users with consistently low engagement metrics, particularly low watch hours, should be targeted. These promotions could range from temporary discounts on their subscription, exclusive access to new content, or bundled offers that encourage deeper exploration of the platform's features. The goal is to re-engage these at-risk users and provide them with an additional incentive to stay.
2.	Encourage more watch time via personalized recommendations and engagement nudges.
Since higher watch hours are strongly correlated with retention, strategies to increase user engagement with content are paramount. This can be achieved through highly sophisticated personalized recommendation systems that leverage viewing history and preferences to suggest relevant content, making content discovery seamless and compelling. Beyond passive recommendations, the platform could implement active engagement nudges. These might include push notifications about new episodes of favorite shows, reminders about unfinished content, or even curated playlists based on specific moods or interests. The objective is to consistently draw users back to the platform and encourage deeper, more frequent content consumption.
3.	Prioritize outreach to mobile-first users, possibly focusing on improving app UX.
The finding that mobile-dominant users churn slightly more suggests that the mobile experience might have subtle friction points. Therefore, it is recommended to prioritize outreach and analysis specifically for this segment. This involves not just generic communications but potentially surveys or user testing focused on the mobile application's User Experience (UX). Understanding whether there are issues with app performance, navigation, content discovery on smaller screens, or specific feature accessibility could uncover critical areas for improvement. Enhancing the mobile app's usability, stability, and content delivery could directly address this segment's higher churn propensity, converting a potential weakness into a strength.
These recommendations are not exhaustive but represent high-impact areas where immediate strategic interventions can be implemented to significantly improve user retention and contribute to the platform's long-term success.

8. Data Issues or Risks: Acknowledging Limitations and Future Considerations
No data analysis is without its challenges, and acknowledging potential data issues or risks is crucial for maintaining transparency and informing future iterations of this project.
One common challenge encountered was the presence of missing feature values for some users. While we addressed this primarily through imputation using SimpleImputer(strategy='mean'), it's important to recognize the limitations of this approach. Imputing with the mean can reduce variance in the dataset and may not always accurately reflect the true underlying values, especially if the missingness is not completely random. For future analyses, exploring more sophisticated imputation techniques, such as K-Nearest Neighbors (KNN) imputation or regression imputation, could potentially yield more robust model performance by preserving more of the original data distribution.
Another significant issue was class imbalance: the number of churned users was substantially fewer than active users. In typical streaming platforms, churn rates are often low, making the "churn" class the minority. If not addressed, this imbalance can lead to models that are biased towards the majority class (non-churners), resulting in poor predictive performance on the minority class (actual churners), which is the class of primary interest for retention efforts. We explicitly addressed this via stratified sampling. During model training and evaluation, stratified sampling ensures that the proportion of churned and non-churned users in each fold of cross-validation (or in the training and testing sets) is maintained at the same ratio as the original dataset. This helps the model learn from both classes more effectively and provides a more reliable assessment of its performance on the minority class. Other techniques like oversampling (e.g., SMOTE) or undersampling could also be considered in future work.
Finally, a critical risk constantly considered throughout the project was potential data leakage. Specifically, the last_active_date column, if used directly in its raw form, could lead to leakage. For instance, if a user's last_active_date was very recent but they subsequently churned, using this 'future' information during training could artificially inflate model performance. Our approach mitigated this by carefully calculating tenure_days as a static feature based on an observed last active date (e.g., at the time the dataset was extracted) and ensuring no forward-looking information was inadvertently included. The principle last_active_date must not be used directly as a feature in its raw form for churn prediction, as churn itself is determined by a user ceasing activity after a certain point, was rigorously adhered to.
Addressing these data issues and continuously refining our data preparation and modeling techniques will be key to enhancing the accuracy and reliability of our churn prediction system in the long term, ensuring it remains a valuable asset for our retention efforts.

